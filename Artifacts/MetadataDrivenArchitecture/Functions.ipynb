{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1614cf-c656-4915-a7f8-86420fdcc170",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T, functions as F, Row\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from delta.tables import DeltaTable\n",
    "import re\n",
    "from pyspark.errors import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b095cf15-c124-40c3-8ca0-2cca79c22eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_sqlmi_table(databaseName: str, schemaName: str, tableName: str) -> DataFrame:\n",
    "    #JDBC Connection to Dev MI - Run this first to create connection strings \n",
    "    akv_scope=f\"\"\"{azure-key-vault-scope-to-get-username-and-password}\"\"\"\n",
    "\n",
    "    jdbcHostName = \"{hostname-of-sql-mi-server}.database.windows.net\"\n",
    "    jdbcDatabase = databaseName\n",
    "    jdbcPort = 1433\n",
    "    jdbcUsername = dbutils.secrets.get(scope = akv_scope, key = \"{secret-key-to-fetch-username}\")  #User name and password are stored in Azure keyvault\n",
    "    jdbcPassword = dbutils.secrets.get(scope = akv_scope, key = \"{secret-key-to-fetch-password}\") \n",
    "    jdbcDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "    jdbcUrl = f\"jdbc:sqlserver://{jdbcHostName}:{jdbcPort};databaseName={jdbcDatabase};user={jdbcUsername};password={jdbcPassword}\"\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", jdbcUrl)\n",
    "        .option(\"dbtable\", f\"{schemaName}.{tableName}\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cee6a93c-78b8-4c95-a7de-ddc213c8a933",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=T.StringType())\n",
    "def get_delta_datatype(srcDb, dataType, precision, scale):\n",
    "    sqlmi_to_delta_datatype_map  = {\n",
    "        \"bigint\": \"long\",\n",
    "        \"binary\": \"binary\",\n",
    "        \"bit\": \"boolean\",\n",
    "        \"char\": \"string\",\n",
    "        \"date\": \"date\",\n",
    "        \"datetime\": \"timestamp\",\n",
    "        \"datetime2\": \"timestamp\",\n",
    "        \"datetimeoffset\": \"string\",\n",
    "        \"decimal\": f\"decimal({precision}, {scale})\",\n",
    "        \"float\": \"double\",\n",
    "        \"geography\": \"string\",\n",
    "        \"geometry\": \"string\",\n",
    "        \"hierarchyid\": \"string\",\n",
    "        \"image\": \"binary\",\n",
    "        \"int\": \"int\",\n",
    "        \"money\": \"decimal(19,4)\",\n",
    "        \"nchar\": \"string\",\n",
    "        \"ntext\": \"string\",\n",
    "        \"numeric\": f\"decimal({precision}, {scale})\",\n",
    "        \"nvarchar\": \"string\",\n",
    "        \"real\": \"float\",\n",
    "        \"smalldatetime\": \"timestamp\",\n",
    "        \"smallint\": \"short\",\n",
    "        \"smallmoney\": \"decimal(10,4)\",\n",
    "        \"sql_variant\": \"string\",\n",
    "        \"text\": \"string\",\n",
    "        \"time\": \"string\",\n",
    "        \"timestamp\": \"timestamp\",\n",
    "        \"tinyint\": \"byte\",\n",
    "        \"uniqueidentifier\": \"string\",\n",
    "        \"varbinary\": \"binary\",\n",
    "        \"varchar\": \"string\",\n",
    "        \"xml\": \"string\",\n",
    "    }\n",
    "\n",
    "    if (srcDb == \"SQL MI\"):\n",
    "        return sqlmi_to_delta_datatype_map.get(dataType, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cce0486-b900-4e58-bd9f-954aeab9d833",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_catalog(\n",
    "    destCatalogName: str,\n",
    "):\n",
    "    ddl = f\"\"\"\n",
    "    CREATE CATALOG {destCatalogName}\n",
    "    \"\"\"\n",
    "    spark.sql(ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb36b005-9003-4eab-9f16-63122812e412",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_schema(\n",
    "    destCatalogName: str,\n",
    "    destSchemaName: str,\n",
    "):\n",
    "    ddl = f\"\"\"\n",
    "    CREATE DATABASE {destCatalogName}.{destSchemaName}\n",
    "    \"\"\"\n",
    "    spark.sql(ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a8737d0-d515-4add-8677-ea79c52fa3a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_meta_tablemappings(\n",
    "    destCatalogName: str,\n",
    "    destSchemaName: str,\n",
    "):\n",
    "    ddl = f\"\"\"\n",
    "    CREATE TABLE {destCatalogName}.{destSchemaName}.meta_tablemappings (\n",
    "        SourceDatabaseName STRING,\n",
    "        SourceSchemaName STRING,\n",
    "        SourceTableName STRING,\n",
    "        DestinationCatalogName STRING,\n",
    "        DestinationSchemaName STRING,\n",
    "        DestinationTableName STRING,\n",
    "        SourceColumnName STRING,\n",
    "        DestinationColumnName STRING,\n",
    "        SourceDataType STRING,\n",
    "        DestinationDataType STRING\n",
    "    ) USING DELTA;\n",
    "    \"\"\"\n",
    "    spark.sql(ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dbd3b6f-2af8-4a9a-8b1e-d8c55e110c3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_meta_dataloadingmetrics(\n",
    "    destCatalogName: str,\n",
    "    destSchemaName: str,\n",
    "):\n",
    "    ddl = f\"\"\"\n",
    "    CREATE TABLE {destCatalogName}.{destSchemaName}.meta_dataloadingmetrics (\n",
    "        ID BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1),\n",
    "        SourceDatabaseName STRING,\n",
    "        SourceSchemaName STRING,\n",
    "        SourceTableName STRING,\n",
    "        DestinationCatalogName STRING,\n",
    "        DestinationSchemaName STRING,\n",
    "        DestinationTableName STRING,\n",
    "        IncrementalLoadFlag BOOLEAN,\n",
    "        ReRunFlag BOOLEAN,\n",
    "        RunDays INT,\n",
    "        WatermarkColumn STRING\n",
    "    ) USING DELTA;\n",
    "    \"\"\"\n",
    "    spark.sql(ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e856d68f-97fe-43f0-a6e5-0bd77b51e310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_meta_audittable(\n",
    "    destCatalogName: str,\n",
    "    destSchemaName: str,\n",
    "):\n",
    "    ddl = f\"\"\"\n",
    "    CREATE TABLE {destCatalogName}.{destSchemaName}.meta_audittable (\n",
    "        ID BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1),\n",
    "        DestinationCatalogName STRING,\n",
    "        DestinationSchemaName STRING,\n",
    "        DestinationTableName STRING,\n",
    "        LastProcessedTime TIMESTAMP,\n",
    "        SourceRowCount STRING,\n",
    "        DestinationRowCount STRING\n",
    "    ) USING DELTA;\n",
    "    \"\"\"\n",
    "    spark.sql(ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53423c4-b9f4-41a7-a40a-3584d67894fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_delta_table(\n",
    "    destCatalogName: str,\n",
    "    destSchemaName: str,\n",
    "    destTableName: str,\n",
    "):\n",
    "    ddl = get_ddl_string(destCatalogName, destSchemaName, destTableName)\n",
    "    spark.sql(ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a3b8b83-99ce-4431-a70f-787953d12b81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " def get_ddl_string(\n",
    "     destCatalogName: str,\n",
    "     destSchemaName: str,\n",
    "     destTableName: str,\n",
    " ) -> str:\n",
    "     columns = (\n",
    "         spark.read.format(\"delta\")\n",
    "         .table(f\"{destCatalogName}.{destSchemaName}.meta_tablemappings\")\n",
    "         .filter(\n",
    "             (F.col(\"DestinationCatalogName\") == destCatalogName)\n",
    "             & (F.col(\"DestinationSchemaName\") == destSchemaName)\n",
    "             & (F.col(\"DestinationTableName\") == destTableName)\n",
    "         )\n",
    "         .collect()\n",
    "     )\n",
    "\n",
    "     ddl = (\n",
    "         f\"CREATE TABLE {destCatalogName}.{destSchemaName}.{destTableName} (\"\n",
    "     )\n",
    "     for i, column in enumerate(columns):\n",
    "         if i != len(columns) - 1:\n",
    "             ddl += f\"{column['DestinationColumnName']} {column['DestinationDataType']},\"\n",
    "         else:\n",
    "             ddl += f\"{column['DestinationColumnName']} {column['DestinationDataType']}\"\n",
    "     ddl += \") USING DELTA;\"\n",
    "\n",
    "     return ddl\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed339cb1-0d21-46a8-90c3-6cb707bde892",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_meta_tablemappings(\n",
    "    srcDatabaseName: str,\n",
    "    srcSchemaName: str,\n",
    "    srcTableName: str,\n",
    "    destCatalogName: str,\n",
    "    destSchemaName: str,\n",
    "    destTableName: str,\n",
    "    metaCatalogName: str,\n",
    "    metaSchemaName: str,\n",
    ") -> dict:\n",
    "    df = load_sqlmi_table(srcDatabaseName, \"INFORMATION_SCHEMA\", \"COLUMNS\")\n",
    "\n",
    "    df = (\n",
    "        df.filter(\n",
    "            (F.col(\"Table_Catalog\") == srcDatabaseName)\n",
    "            & (F.col(\"Table_Schema\") == srcSchemaName)\n",
    "            & (F.col(\"Table_Name\") == srcTableName)\n",
    "        )\n",
    "        .withColumnRenamed(\"Table_Catalog\", \"SourceDatabaseName\")\n",
    "        .withColumnRenamed(\"Table_Schema\", \"SourceSchemaName\")\n",
    "        .withColumnRenamed(\"Table_Name\", \"SourceTableName\")\n",
    "        .withColumnRenamed(\"Column_Name\", \"SourceColumnName\")\n",
    "        .withColumnRenamed(\"Data_Type\", \"SourceDataType\")\n",
    "        .withColumn(\n",
    "            \"DestinationColumnName\",\n",
    "            F.regexp_replace(F.col(\"SourceColumnName\"), \"[ /()&?$]\", \"\"),\n",
    "        )\n",
    "       .withColumn(\n",
    "            \"DestinationColumnName\",\n",
    "            F.regexp_replace(F.col(\"DestinationColumnName\"),\"[%]\",\"perc\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"DestinationColumnName\",\n",
    "            F.regexp_replace(F.col(\"DestinationColumnName\"), \"[-.]\", \"_\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"DestinationDataType\",\n",
    "            get_delta_datatype(\n",
    "                F.lit(\"SQL MI\"), \"SourceDataType\", \"NUMERIC_PRECISION\", \"NUMERIC_SCALE\"\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\"DestinationCatalogName\", F.lit(destCatalogName).cast(\"string\"))\n",
    "        .withColumn(\"DestinationSchemaName\", F.lit(destSchemaName).cast(\"string\"))\n",
    "        .withColumn(\"DestinationTableName\", F.lit(destTableName).cast(\"string\"))\n",
    "        .select(\n",
    "            \"SourceDatabaseName\",\n",
    "            \"SourceSchemaName\",\n",
    "            \"SourceTableName\",\n",
    "            \"SourceColumnName\",\n",
    "            \"SourceDataType\",\n",
    "            \"DestinationColumnName\",\n",
    "            \"DestinationDataType\",\n",
    "            \"DestinationCatalogName\",\n",
    "            \"DestinationSchemaName\",\n",
    "            \"DestinationTableName\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    keyColumns = [\n",
    "        \"SourceDatabaseName\",\n",
    "        \"SourceSchemaName\",\n",
    "        \"SourceTableName\",\n",
    "        \"SourceColumnName\",\n",
    "    ]\n",
    "\n",
    "    updateColumns = [\n",
    "        \"DestinationColumnName\",\n",
    "        \"DestinationDataType\",\n",
    "        \"DestinationCatalogName\",\n",
    "        \"DestinationSchemaName\",\n",
    "        \"DestinationTableName\",\n",
    "    ]\n",
    "\n",
    "    delta_table = DeltaTable.forName(\n",
    "        spark, f\"{metaCatalogName}.{metaSchemaName}.meta_tablemappings\"\n",
    "    )\n",
    "\n",
    "    (\n",
    "        delta_table.alias(\"target\")\n",
    "        .merge(\n",
    "            df.alias(\"source\"),\n",
    "            \" and \".join(f\"target.{c} = source.{c}\" for c in keyColumns),\n",
    "        )\n",
    "        .whenMatchedUpdateAll(\n",
    "            \" or \".join([f\"target.`{c}` != source.`{c}`\" for c in updateColumns])\n",
    "        )\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    return get_record_stats(destCatalogName, destSchemaName, \"meta_tablemappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036425ed-f783-4f58-82ab-caddf8007eee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_meta_dataloadingmetrics(\n",
    "    srcDatabaseName: str,\n",
    "    srcSchemaName: str,\n",
    "    srcTableName: str,\n",
    "    destCatalogName: str,\n",
    "    destSchemaName: str,\n",
    "    destTableName: str,\n",
    "    incLoadFlag: int,\n",
    "    reRunFlag: int,\n",
    "    runDays: int,\n",
    "    watermarkColumn: str,\n",
    "    metaCatalogName: str,\n",
    "    metaSchemaName: str,\n",
    ") -> dict:\n",
    "\n",
    "    schema = T.StructType(\n",
    "        [\n",
    "            T.StructField(\"SourceDatabaseName\", T.StringType(), True),\n",
    "            T.StructField(\"SourceSchemaName\", T.StringType(), True),\n",
    "            T.StructField(\"SourceTableName\", T.StringType(), True),\n",
    "            T.StructField(\"DestinationCatalogName\", T.StringType(), True),\n",
    "            T.StructField(\"DestinationSchemaName\", T.StringType(), True),\n",
    "            T.StructField(\"DestinationTableName\", T.StringType(), True),\n",
    "            T.StructField(\"IncrementalLoadFlag\", T.BooleanType(), True),\n",
    "            T.StructField(\"ReRunFlag\", T.BooleanType(), True),\n",
    "            T.StructField(\"RunDays\", T.IntegerType(), True),\n",
    "            T.StructField(\"WatermarkColumn\", T.StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if watermarkColumn is not None:\n",
    "        watermarkColumn = str(re.sub(r\"[ /()&]\", \"\", watermarkColumn))\n",
    "        watermarkColumn = str(watermarkColumn.replace(\"-\", \"_\"))\n",
    "    else:\n",
    "        watermarkColumn = str(None)\n",
    "\n",
    "    data = [\n",
    "        Row(\n",
    "            srcDatabaseName,\n",
    "            srcSchemaName,\n",
    "            srcTableName,\n",
    "            destCatalogName,\n",
    "            destSchemaName,\n",
    "            destTableName,\n",
    "            bool(incLoadFlag),\n",
    "            bool(reRunFlag),\n",
    "            int(runDays),\n",
    "            watermarkColumn,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    keyColumns = [\n",
    "        \"SourceDatabaseName\",\n",
    "        \"SourceSchemaName\",\n",
    "        \"SourceTableName\",\n",
    "    ]\n",
    "\n",
    "    updateColumns = [\n",
    "        \"DestinationCatalogName\",\n",
    "        \"DestinationSchemaName\",\n",
    "        \"DestinationTableName\",\n",
    "        \"IncrementalLoadFlag\",\n",
    "        \"ReRunFlag\",\n",
    "        \"RunDays\",\n",
    "        \"WatermarkColumn\",\n",
    "    ]\n",
    "\n",
    "    valuesColumns = {\n",
    "        f\"target.{col}\": f\"source.{col}\" for col in keyColumns + updateColumns\n",
    "    }\n",
    "    \n",
    "    setColumns = {f\"target.{col}\": f\"source.{col}\" for col in updateColumns}\n",
    "\n",
    "    delta_table = DeltaTable.forName(\n",
    "        spark, f\"{metaCatalogName}.{metaSchemaName}.meta_dataloadingmetrics\"\n",
    "    )\n",
    "\n",
    "    (\n",
    "        delta_table.alias(\"target\")\n",
    "        .merge(\n",
    "            df.alias(\"source\"),\n",
    "            \" and \".join(f\"target.{c} = source.{c}\" for c in keyColumns),\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            \" or \".join([f\"target.`{c}` != source.`{c}`\" for c in updateColumns]),\n",
    "            set=setColumns,\n",
    "        )\n",
    "        .whenNotMatchedInsert(values=valuesColumns)\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    return get_record_stats(destCatalogName, destSchemaName, \"meta_dataloadingmetrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d22462d-cc99-4e91-b426-d1f7b3030196",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_meta_audittable(destCatalogName: str, destSchemaName: str, destTableName: str, srcRowCount: int):\n",
    "    delta_table = DeltaTable.forName(\n",
    "        spark, f\"{destCatalogName}.{destSchemaName}.{destTableName}\"\n",
    "    )\n",
    "\n",
    "    lastProcessedTime = delta_table.history(1).first().timestamp\n",
    "\n",
    "    destRowCount = delta_table.toDF().count()\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"DestinationCatalogName\", T.StringType(), True),\n",
    "        T.StructField(\"DestinationSchemaName\", T.StringType(), True),\n",
    "        T.StructField(\"DestinationTableName\", T.StringType(), True),\n",
    "        T.StructField(\"LastProcessedTime\", T.TimestampType(), True),\n",
    "        T.StructField(\"SourceRowCount\", T.StringType(), True),\n",
    "        T.StructField(\"DestinationRowCount\", T.StringType(), True)\n",
    "    ])\n",
    "\n",
    "    data = [\n",
    "        Row(destCatalogName, destSchemaName, destTableName, lastProcessedTime, srcRowCount, destRowCount),\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    keyColumns = [\n",
    "        \"DestinationCatalogName\",\n",
    "        \"DestinationSchemaName\",\n",
    "        \"DestinationTableName\",\n",
    "    ]\n",
    "\n",
    "    updateColumns = [\n",
    "        \"LastProcessedTime\",\n",
    "        \"SourceRowCount\",\n",
    "        \"DestinationRowCount\",\n",
    "    ]\n",
    "\n",
    "    valuesColumns = {\n",
    "        f\"target.{col}\": f\"source.{col}\" for col in keyColumns + updateColumns\n",
    "    }\n",
    "    \n",
    "    setColumns = {f\"target.{col}\": f\"source.{col}\" for col in updateColumns}\n",
    "\n",
    "    meta_auditTable = DeltaTable.forName(\n",
    "        spark, f\"{destCatalogName}.{destSchemaName}.meta_audittable\"\n",
    "    )\n",
    "\n",
    "    (\n",
    "        meta_auditTable.alias(\"target\")\n",
    "        .merge(\n",
    "            df.alias(\"source\"),\n",
    "            \" and \".join(f\"target.{c} = source.{c}\" for c in keyColumns),\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            \" or \".join([f\"target.`{c}` != source.`{c}`\" for c in updateColumns]),\n",
    "            set=setColumns,\n",
    "        )\n",
    "        .whenNotMatchedInsert(values=valuesColumns)\n",
    "        .execute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f87b0e-2ac1-4e82-ae40-1b9ac4c2fc37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_delta_table(\n",
    "    srcDatabaseName: str,\n",
    "    srcSchemaName: str,\n",
    "    srcTableName: str,\n",
    "    destCatalogName: str,\n",
    "    destSchemaName: str,\n",
    "    destTableName: str,\n",
    "    incLoadFlag: bool,\n",
    "    reRunFlag: bool,\n",
    "    runDays: int,\n",
    "    watermarkColumn: str,\n",
    ") -> dict:\n",
    "    if incLoadFlag:\n",
    "        return incremental_load(\n",
    "            srcDatabaseName,\n",
    "            srcSchemaName,\n",
    "            srcTableName,\n",
    "            destCatalogName,\n",
    "            destSchemaName,\n",
    "            destTableName,\n",
    "            incLoadFlag,\n",
    "            reRunFlag,\n",
    "            runDays,\n",
    "            watermarkColumn,\n",
    "        )\n",
    "    else:\n",
    "        return full_load(\n",
    "            srcDatabaseName,\n",
    "            srcSchemaName,\n",
    "            srcTableName,\n",
    "            destCatalogName,\n",
    "            destSchemaName,\n",
    "            destTableName,\n",
    "            incLoadFlag,\n",
    "            reRunFlag,\n",
    "            runDays,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02be9836-c8f1-40d3-92ab-441667f9dcaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def incremental_load(\n",
    "    srcDatabaseName: str,\n",
    "    srcSchemaName: str,\n",
    "    srcTableName: str,\n",
    "    destCatalogName: str,\n",
    "    destSchemaName: str,\n",
    "    destTableName: str,\n",
    "    incLoadFlag: bool,\n",
    "    reRunFlag: bool,\n",
    "    runDays: int,\n",
    "    watermarkColumn: str,\n",
    ") -> dict:\n",
    "\n",
    "    targetCount = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(f\"{destCatalogName}.{destSchemaName}.{destTableName}\")\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    if targetCount == 0 or reRunFlag:\n",
    "        return full_load(\n",
    "            srcDatabaseName,\n",
    "            srcSchemaName,\n",
    "            srcTableName,\n",
    "            destCatalogName,\n",
    "            destSchemaName,\n",
    "            destTableName,\n",
    "            incLoadFlag,\n",
    "            reRunFlag,\n",
    "            runDays,\n",
    "        )\n",
    "    else:\n",
    "        runDaysDate = spark.sql(\n",
    "            f\"SELECT CAST(REPLACE(current_date() - {runDays}, '-', '') AS INT)\"\n",
    "        ).first()[0]\n",
    "\n",
    "        spark.sql(\n",
    "            f\"DELETE FROM {destCatalogName}.{destSchemaName}.{destTableName} WHERE {watermarkColumn} >= {runDaysDate}\"\n",
    "        )\n",
    "\n",
    "        df = load_sqlmi_table(srcDatabaseName, srcSchemaName, srcTableName).filter(\n",
    "            F.col(f\"{watermarkColumn}\") >= runDaysDate\n",
    "        )\n",
    "\n",
    "        columnTypeMapping = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .table(f\"{destCatalogName}.{destSchemaName}.meta_tablemappings\")\n",
    "            .filter(\n",
    "                (F.col(\"DestinationCatalogName\") == destCatalogName)\n",
    "                & (F.col(\"DestinationSchemaName\") == destSchemaName)\n",
    "                & (F.col(\"DestinationTableName\") == destTableName)\n",
    "            )\n",
    "            .select(\n",
    "                \"SourceColumnName\",\n",
    "                \"DestinationColumnName\",\n",
    "                \"DestinationDataType\",\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        selectCols = list()\n",
    "\n",
    "        for c in columnTypeMapping:\n",
    "            df = df.withColumn(\n",
    "                c[\"DestinationColumnName\"],\n",
    "                F.col(c[\"SourceColumnName\"]).cast(c[\"DestinationDataType\"]),\n",
    "            )\n",
    "            selectCols.append(c[\"DestinationColumnName\"])\n",
    "\n",
    "        try:\n",
    "            df.count()\n",
    "        except Exception as e:\n",
    "            df = load_sqlmi_table(srcDatabaseName, srcSchemaName, srcTableName)\n",
    "            for c in columnTypeMapping:\n",
    "                df = df.withColumnRenamed(\n",
    "                    c[\"SourceColumnName\"],\n",
    "                    c[\"DestinationColumnName\"],\n",
    "                )\n",
    "            try:\n",
    "                df.count()\n",
    "            except Exception as e:\n",
    "                return \"[ERROR]: Datatype casting and column renaming failed.\"\n",
    "\n",
    "        df.select(selectCols).write.format(\"delta\").mode(\"append\").option(\n",
    "            \"mergeSchema\", \"true\"\n",
    "        ).saveAsTable(f\"{destCatalogName}.{destSchemaName}.{destTableName}\")\n",
    "\n",
    "        srcRowCount = df.count()\n",
    "\n",
    "        upsert_meta_audittable(\n",
    "            destCatalogName, destSchemaName, destTableName, srcRowCount\n",
    "        )\n",
    "\n",
    "        return get_record_stats(destCatalogName, destSchemaName, destTableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbf0738-b080-4ce0-b880-78fabb93e8a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def full_load(\n",
    "    srcDatabaseName: str,\n",
    "    srcSchemaName: str,\n",
    "    srcTableName: str,\n",
    "    destCatalogName: str,\n",
    "    destSchemaName: str,\n",
    "    destTableName: str,\n",
    "    incLoadFlag: bool,\n",
    "    reRunFlag: bool,\n",
    "    runDays: int,\n",
    ") -> dict:\n",
    "\n",
    "    targetCount = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(f\"{destCatalogName}.{destSchemaName}.{destTableName}\")\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    if targetCount == 0 or reRunFlag:\n",
    "        df = load_sqlmi_table(srcDatabaseName, srcSchemaName, srcTableName)\n",
    "\n",
    "        columnTypeMapping = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .table(f\"{destCatalogName}.{destSchemaName}.meta_tablemappings\")\n",
    "            .filter(\n",
    "                (F.col(\"DestinationCatalogName\") == destCatalogName)\n",
    "                & (F.col(\"DestinationSchemaName\") == destSchemaName)\n",
    "                & (F.col(\"DestinationTableName\") == destTableName)\n",
    "            )\n",
    "            .select(\n",
    "                \"SourceColumnName\",\n",
    "                \"DestinationColumnName\",\n",
    "                \"DestinationDataType\",\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        selectCols = list()\n",
    "\n",
    "        for c in columnTypeMapping:\n",
    "            df = df.withColumn(\n",
    "                c[\"DestinationColumnName\"],\n",
    "                F.col(c[\"SourceColumnName\"]).cast(c[\"DestinationDataType\"]),\n",
    "            )\n",
    "            selectCols.append(c[\"DestinationColumnName\"])\n",
    "\n",
    "        try:\n",
    "            df.count()\n",
    "        except Exception as e:\n",
    "            df = load_sqlmi_table(srcDatabaseName, srcSchemaName, srcTableName)\n",
    "            for c in columnTypeMapping:\n",
    "                df = df.withColumnRenamed(\n",
    "                    c[\"SourceColumnName\"],\n",
    "                    c[\"DestinationColumnName\"],\n",
    "                )\n",
    "            try:\n",
    "                df.count()\n",
    "            except Exception as e:\n",
    "                return \"[ERROR]: Datatype casting and column renaming failed.\"\n",
    "\n",
    "        df.select(selectCols).write.format(\"delta\").mode(\"overwrite\").option(\n",
    "            \"ōverwriteSchema\", \"true\"\n",
    "        ).saveAsTable(f\"{destCatalogName}.{destSchemaName}.{destTableName}\")\n",
    "\n",
    "        srcRowCount = df.count()\n",
    "\n",
    "        upsert_meta_audittable(\n",
    "            destCatalogName, destSchemaName, destTableName, srcRowCount\n",
    "        )\n",
    "\n",
    "        return get_record_stats(destCatalogName, destSchemaName, destTableName)\n",
    "    else:\n",
    "        return {\n",
    "            \"table\": f\"{destCatalogName}.{destSchemaName}.{destTableName}\",\n",
    "            \"sourceRows\": 0,\n",
    "            \"rowsInserted\": 0,\n",
    "            \"rowsUpdated\": 0,\n",
    "            \"rowsDeleted\": 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a245b80-06c2-4575-937b-9274f7f63e19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def catalog_exists(catalog: str) -> bool:\n",
    "    catalog_exists = (\n",
    "        spark.sql(\"SHOW CATALOGS\").filter(f\"catalog = '{catalog}'\").count() > 0\n",
    "    )\n",
    "\n",
    "    return catalog_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "158fa5f3-a012-4153-82b0-39f224a327dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def schema_exists(catalog: str, schema: str) -> bool:\n",
    "    schema_exists = (\n",
    "        spark.sql(f\"SHOW DATABASES IN {catalog}\")\n",
    "        .filter(f\"databaseName = '{schema}'\")\n",
    "        .count()\n",
    "        > 0\n",
    "    )\n",
    "\n",
    "    return schema_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d41f47-fc77-4e48-818c-fec41aa85452",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def table_exists(catalog: str, schema: str, table: str) -> bool:\n",
    "    \n",
    "    catalog_exists = (\n",
    "        spark.sql(\"SHOW CATALOGS\").filter(f\"catalog = '{catalog}'\").count() > 0\n",
    "    )\n",
    "\n",
    "    if not catalog_exists:\n",
    "        return False\n",
    "    \n",
    "    schema_exists = (\n",
    "        spark.sql(f\"SHOW DATABASES IN {catalog}\")\n",
    "        .filter(f\"databaseName = '{schema}'\")\n",
    "        .count()\n",
    "        > 0\n",
    "    )\n",
    "\n",
    "    if not schema_exists:\n",
    "        return False\n",
    "    \n",
    "    table_exists = (\n",
    "        spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\")\n",
    "        .filter(f\"tableName = '{table.lower()}'\")\n",
    "        .count()\n",
    "        > 0\n",
    "    )\n",
    "\n",
    "    return table_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5be3930-ee14-4269-b03d-a0df5ff5043f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_record_stats(\n",
    "    destCatalogName: str, destSchemaName: str, destTableName: str\n",
    ") -> dict:\n",
    "    \n",
    "    delta_table = DeltaTable.forName(\n",
    "        spark, f\"{destCatalogName}.{destSchemaName}.{destTableName}\"\n",
    "    )\n",
    "\n",
    "    recordStats = delta_table.history(1).first()\n",
    "\n",
    "    if recordStats.operation == \"CREATE OR REPLACE TABLE AS SELECT\":\n",
    "        return {\n",
    "            \"table\": f\"{destCatalogName}.{destSchemaName}.{destTableName}\",\n",
    "            \"version\": recordStats.version,\n",
    "            \"sourceRows\": recordStats.operationMetrics[\"numOutputRows\"],\n",
    "            \"rowsInserted\": recordStats.operationMetrics[\"numOutputRows\"],\n",
    "            \"rowsUpdated\": 0,\n",
    "            \"rowsDeleted\": 0,\n",
    "        }\n",
    "    elif recordStats.operation == \"MERGE\":\n",
    "        return {\n",
    "            \"table\": f\"{destCatalogName}.{destSchemaName}.{destTableName}\",\n",
    "            \"version\": recordStats.version,\n",
    "            \"sourceRows\": recordStats.operationMetrics[\"numSourceRows\"],\n",
    "            \"rowsInserted\": recordStats.operationMetrics[\"numTargetRowsInserted\"],\n",
    "            \"rowsUpdated\": recordStats.operationMetrics[\"numTargetRowsUpdated\"],\n",
    "            \"rowsDeleted\": recordStats.operationMetrics[\"numTargetRowsDeleted\"],\n",
    "        }\n",
    "    elif recordStats.operation == \"WRITE\":\n",
    "        return {\n",
    "            \"table\": f\"{destCatalogName}.{destSchemaName}.{destTableName}\",\n",
    "            \"version\": recordStats.version,\n",
    "            \"sourceRows\": recordStats.operationMetrics[\"numOutputRows\"],\n",
    "            \"rowsInserted\": recordStats.operationMetrics[\"numOutputRows\"],\n",
    "            \"rowsUpdated\": 0,\n",
    "            \"rowsDeleted\": 0,\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"table\": f\"{destCatalogName}.{destSchemaName}.{destTableName}\",\n",
    "            \"version\": recordStats.version,\n",
    "            \"sourceRows\": -1,\n",
    "            \"rowsInserted\": -1,\n",
    "            \"rowsUpdated\": -1,\n",
    "            \"rowsDeleted\": -1,\n",
    "            \"error\": \"Unknown operation\",\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4287211267892239,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
